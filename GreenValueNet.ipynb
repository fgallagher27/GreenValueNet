{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GreenValueNet\n",
    "\n",
    "This notebook contains the code needed to execute the GreenValueNet hedonic pricing neural network. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set up and data loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from data_load_funcs import get_params, load_data_catalogue\n",
    "from processing_funcs import process_data, normalise_values\n",
    "from model_funcs import *\n",
    "\n",
    "params = get_params()\n",
    "data_catalogue = load_data_catalogue()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you do not have a file called `dataset.csv` in the `data/interim_files` folder the following cell will generate this folder and generate summary statistics. The data processing happens locally and invovles large datasets with spatial components so can take quite several hours - please be pateint! If you already have the file, it will be read in and summary statistics are generated. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = process_data(data_catalogue, params)\n",
    "\n",
    "# show summary stats\n",
    "summary_stats = dataset.describe().transpose()[['mean', 'std', 'max', 'min']]\n",
    "summary_stats.columns = ['Mean', 'Std Dev', 'Maximum', 'Minimum']\n",
    "print(summary_stats)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we normalise any non-encoded variables to increase speed of learning of algorithm and convert the dataset to an array of inputs, and an associated output array."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "norm_cols = [col for col in dataset.columns if col not in params['non_norm_cols']]\n",
    "for col in norm_cols:\n",
    "    dataset[col] = normalise_values(dataset[col])\n",
    "\n",
    "# creates an arry of shape m, x, y\n",
    "x, y = create_x_y_arr(dataset, params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dataset is then split into train, dev and test sets using sci-kit learn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, x_dev, x_test, y_train, y_dev, y_test = split_to_test_dev_train(\n",
    "    x,\n",
    "    y,\n",
    "    params['dev_size'],\n",
    "    params['test_size'],\n",
    "    prop=False\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Benchmarking\n",
    "\n",
    "To evaluate the performance of my neural network I will run random forest and XGBoost regressions as baseline models. I will then build 2 alternative models: a deep neural network and a bayesian model. We optimise based on the mean squared error (MSE) but and report this as our measure of performance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Forest\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run baseline random forest regression using scikit-learn\n",
    "rfr_model = random_forest_reg(\n",
    "    x_train,\n",
    "    y_train,\n",
    "    tuning=False\n",
    ")\n",
    "\n",
    "# now run with grid search to tune parameters\n",
    "rfr_tuned  =  random_forest_reg(\n",
    "    x_train,\n",
    "    y_train,\n",
    "    tuning=True,\n",
    "    tuning_params = params['tuning_dict']['grid']\n",
    ")\n",
    "\n",
    "# generate predictions and measure according to mean squared error\n",
    "rfr_pred, rfr_mse = generate_pred_metric(rfr_model, mean_squared_error, x_dev, y_dev)\n",
    "rfr_t_pred, rfr_t_mse = generate_pred_metric(rfr_tuned, mean_squared_error, x_dev, y_dev)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb_model = boosted_grad_reg(x_train, y_train)\n",
    "xgb_pred, xgb_mse = generate_pred_metric(xgb_model, mean_squared_error, x_dev, y_dev)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neural networks\n",
    "\n",
    "We know build some neural networks. Number of epochs, hidden layers, and nodes in hidden layers is initially set with rules of thumb but then optimiszed using hyperparameter tuning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set epochs to be 3 times number of features\n",
    "epochs = int(x_train.shape[1]) * 3\n",
    "\n",
    "# set n_hidden_units to be mean of input and output layer sizes\n",
    "n_hidden_units = round((x_train.shape[1] + 1) / 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Single Layer Neural Network\n",
    "\n",
    "A single hidden layer with ReLU activation is used with a linear output layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "single_nn = neural_net(\n",
    "    x_train,\n",
    "    y_train,\n",
    "    n_hidden_units = n_hidden_units,\n",
    "    epochs = epochs,\n",
    "    validation_data = (x_dev, y_dev)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "single_nn.history.history"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Deep Neural Network\n",
    "\n",
    "The full model is specified as a deep neural network using layers with ReLU activation functions with a linear activation in the output layer. The choice of number of layers was initially kept small due to computational processing constraints."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "deep_nn = neural_net(\n",
    "    x_train,\n",
    "    y_train,\n",
    "    n_layers = 5,\n",
    "    n_hidden_units = n_hidden_units,\n",
    "    epochs = epochs,\n",
    "    validation_data = (x_dev, y_dev)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save model as tensor object and stick in a folder called outputs\n",
    "model_dir = cwd / \"outputs\" / \"models\"\n",
    "deep_nn.export(model_dir / \"deep_nn.tf\") # check this file ending"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.1"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
