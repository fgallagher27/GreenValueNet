{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GreenValueNet\n",
    "\n",
    "This notebook contains the code needed to execute the GreenValueNet hedonic pricing neural network. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set up and data loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import joblib\n",
    "import matplotlib.pyplot as plt\n",
    "from pathlib import Path\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from data_load_funcs import get_params, load_data_catalogue\n",
    "from processing_funcs import process_data, normalise_values\n",
    "from model_funcs import *\n",
    "\n",
    "cwd = Path.cwd()\n",
    "params = get_params()\n",
    "data_catalogue = load_data_catalogue()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `dataset.csv` file from `data/interim_files` is loaded if it already exists, or constructed if not. To build the dataset please follow the steps in [Annex 1 of the ReadMe file](readme.md#annex-1) to esnure all relevant files have been downloaded. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed dataset already exists.\n",
      "Loading exisitng dataset...\n",
      "                                  Mean    Std Dev       Maximum   Minimum\n",
      "ln_price                     12.393713   0.658136     18.134158  0.116855\n",
      "propertytype                  1.328094   1.085676      3.000000  0.000000\n",
      "oldnew                        0.134435   0.341119      1.000000  0.000000\n",
      "duration                      0.325202   0.468482      2.000000  0.000000\n",
      "current_energy_efficiency    61.056443  12.444568    347.000000  0.000000\n",
      "potential_energy_efficiency   0.805024   0.140106      1.000000  0.000000\n",
      "total_floor_area             91.622326  68.669650  60506.700000  0.000000\n",
      "extension_count               0.495053   0.743755      4.000000  0.000000\n",
      "number_habitable_rooms        4.521529   1.748808    100.000000  0.000000\n",
      "number_heated_rooms           4.437662   1.754007    100.000000  0.000000\n",
      "construction_age_band         4.477500   3.153767     10.000000 -1.000000\n",
      "coastline_dist               57.173176  38.518186    144.036327  0.004453\n",
      "prim_school_dist              0.846688   0.763455     19.868981  0.000000\n",
      "sec_school_dist               2.576883   2.873106     82.942021  0.000000\n",
      "roads_dist                    1.081792   1.578059     28.161536  0.000346\n",
      "nat_park_dist                77.727576  48.713587    265.390815  0.000000\n",
      "nat_trust_dist               22.852628  21.907702    142.965575  0.000000\n",
      "ttwa_dist                     0.000328   0.009237      0.673083  0.000000\n",
      "dom_builds_share              0.067904   0.052405      0.311189  0.000168\n",
      "garden_share                  0.195073   0.134780      0.628869  0.000000\n",
      "non_dom_builds_share          0.033796   0.040105      0.495773  0.000082\n",
      "path_share                    0.006395   0.006409      0.070791  0.000000\n",
      "greenspace_share              0.512138   0.279680      0.989144  0.012036\n",
      "water_share                   0.027889   0.074272      0.871968  0.000000\n"
     ]
    }
   ],
   "source": [
    "dataset = process_data(data_catalogue, params)\n",
    "\n",
    "# show summary stats\n",
    "summary_stats = dataset.describe().transpose()[['mean', 'std', 'max', 'min']]\n",
    "summary_stats.columns = ['Mean', 'Std Dev', 'Maximum', 'Minimum']\n",
    "print(summary_stats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(12909200, 26)\n",
      "transactionid                  0\n",
      "ln_price                       0\n",
      "postcode                       0\n",
      "propertytype                   0\n",
      "oldnew                         0\n",
      "duration                       0\n",
      "current_energy_efficiency      0\n",
      "potential_energy_efficiency    0\n",
      "total_floor_area               0\n",
      "extension_count                0\n",
      "number_habitable_rooms         0\n",
      "number_heated_rooms            0\n",
      "construction_age_band          0\n",
      "coastline_dist                 0\n",
      "prim_school_dist               0\n",
      "sec_school_dist                0\n",
      "roads_dist                     0\n",
      "nat_park_dist                  0\n",
      "nat_trust_dist                 0\n",
      "ttwa_dist                      0\n",
      "dom_builds_share               0\n",
      "garden_share                   0\n",
      "non_dom_builds_share           0\n",
      "path_share                     0\n",
      "greenspace_share               0\n",
      "water_share                    0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(dataset.shape)\n",
    "print(dataset.isnull().sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we normalise any non-encoded variables to increase speed of learning of algorithm and convert the dataset to an array of inputs, and an associated output array."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "norm_cols = [col for col in dataset.columns if col not in params['non_norm_cols']]\n",
    "for col in norm_cols:\n",
    "    dataset[col] = normalise_values(dataset[col])\n",
    "\n",
    "# creates an input array of shape m,x and an output array of shape m\n",
    "x, y, derivative_index = create_x_y_arr(dataset, params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dataset is then split into train, dev and test sets using sci-kit learn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, x_dev, x_test, y_train, y_dev, y_test = split_to_test_dev_train(\n",
    "    x,\n",
    "    y,\n",
    "    params['dev_size'],\n",
    "    params['test_size'],\n",
    "    prop=False\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Benchmarking\n",
    "\n",
    "To evaluate the performance of my neural network we use random forest and XGBoost regressions as baseline models. We optimise based on the mean squared error (MSE) and report this as our measure of performance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Forest\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run baseline random forest regression using scikit-learn\n",
    "rfr_model = random_forest_reg(\n",
    "    x_train,\n",
    "    y_train,\n",
    "    tuning=False\n",
    ")\n",
    "\n",
    "# now run with grid search to tune parameters\n",
    "# rfr_tuned  =  random_forest_reg(\n",
    "#     x_train,\n",
    "#     y_train,\n",
    "#     tuning=True,\n",
    "#     tuning_params = params['tuning_dict']['grid']\n",
    "# )\n",
    "\n",
    "# generate predictions and measure according to mean squared error\n",
    "rfr_pred, rfr_mse = generate_pred_metric(rfr_model, mean_squared_error, x_dev, y_dev)\n",
    "# rfr_t_pred, rfr_t_mse = generate_pred_metric(rfr_tuned, mean_squared_error, x_dev, y_dev)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb_model = boosted_grad_reg(x_train, y_train)\n",
    "xgb_pred, xgb_mse = generate_pred_metric(xgb_model, mean_squared_error, x_dev, y_dev)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neural networks\n",
    "\n",
    "We know build some neural networks. Number of epochs, hidden layers, and nodes in hidden layers is initially set with rules of thumb but then optimiszed using hyperparameter tuning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set epochs to be 3 times number of features\n",
    "epochs = int(x_train.shape[1]) * 3\n",
    "\n",
    "# set n_hidden_units to be mean of input and output layer sizes\n",
    "n_hidden_units = round((x_train.shape[1] + 1) / 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Single Layer Neural Network\n",
    "\n",
    "A single hidden layer with ReLU activation is used with a linear output layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/69\n",
      "402788/402788 [==============================] - 978s 2ms/step - loss: 0.2600 - mae: 0.3883 - val_loss: 0.2569 - val_mae: 0.3709\n",
      "Epoch 2/69\n",
      "402788/402788 [==============================] - 973s 2ms/step - loss: 0.2654 - mae: 0.3961 - val_loss: 0.2694 - val_mae: 0.3829\n",
      "Epoch 3/69\n",
      "402788/402788 [==============================] - 965s 2ms/step - loss: 0.2677 - mae: 0.3979 - val_loss: 0.2665 - val_mae: 0.3831\n",
      "Epoch 4/69\n",
      "402788/402788 [==============================] - 931s 2ms/step - loss: 0.2677 - mae: 0.3978 - val_loss: 0.2720 - val_mae: 0.3830\n",
      "Epoch 5/69\n",
      "402788/402788 [==============================] - 954s 2ms/step - loss: 0.2676 - mae: 0.3977 - val_loss: 0.2772 - val_mae: 0.4019\n",
      "Epoch 6/69\n",
      "402788/402788 [==============================] - 1175s 3ms/step - loss: 0.2677 - mae: 0.3978 - val_loss: 0.3001 - val_mae: 0.4272\n",
      "Epoch 7/69\n",
      "402788/402788 [==============================] - 821s 2ms/step - loss: 0.2676 - mae: 0.3977 - val_loss: 0.2675 - val_mae: 0.3877\n",
      "Epoch 8/69\n",
      "402788/402788 [==============================] - 859s 2ms/step - loss: 0.2676 - mae: 0.3978 - val_loss: 0.2827 - val_mae: 0.4057\n",
      "Epoch 9/69\n",
      "402788/402788 [==============================] - 914s 2ms/step - loss: 0.2675 - mae: 0.3977 - val_loss: 0.2886 - val_mae: 0.3913\n",
      "Epoch 10/69\n",
      "402788/402788 [==============================] - 819s 2ms/step - loss: 0.2676 - mae: 0.3977 - val_loss: 0.2722 - val_mae: 0.3969\n",
      "Epoch 11/69\n",
      "402788/402788 [==============================] - 764s 2ms/step - loss: 0.2676 - mae: 0.3978 - val_loss: 0.3006 - val_mae: 0.3950\n",
      "Epoch 12/69\n",
      "402788/402788 [==============================] - 767s 2ms/step - loss: 0.2675 - mae: 0.3977 - val_loss: 0.2713 - val_mae: 0.3853\n",
      "Epoch 13/69\n",
      "402788/402788 [==============================] - 760s 2ms/step - loss: 0.2676 - mae: 0.3978 - val_loss: 0.2816 - val_mae: 0.4067\n",
      "Epoch 14/69\n",
      "402788/402788 [==============================] - 736s 2ms/step - loss: 0.2675 - mae: 0.3977 - val_loss: 0.2700 - val_mae: 0.3845\n",
      "Epoch 15/69\n",
      "402788/402788 [==============================] - 741s 2ms/step - loss: 0.2676 - mae: 0.3978 - val_loss: 0.2743 - val_mae: 0.3873\n",
      "Epoch 16/69\n",
      "402788/402788 [==============================] - 802s 2ms/step - loss: 0.2676 - mae: 0.3977 - val_loss: 0.2748 - val_mae: 0.3840\n",
      "Epoch 17/69\n",
      "402788/402788 [==============================] - 760s 2ms/step - loss: 0.2676 - mae: 0.3978 - val_loss: 0.2697 - val_mae: 0.3888\n",
      "Epoch 18/69\n",
      "402788/402788 [==============================] - 752s 2ms/step - loss: 0.2676 - mae: 0.3978 - val_loss: 0.2725 - val_mae: 0.3840\n",
      "Epoch 19/69\n",
      "402788/402788 [==============================] - 762s 2ms/step - loss: 0.2676 - mae: 0.3978 - val_loss: 0.2801 - val_mae: 0.4059\n",
      "Epoch 20/69\n",
      "402788/402788 [==============================] - 752s 2ms/step - loss: 0.2676 - mae: 0.3978 - val_loss: 0.2737 - val_mae: 0.3861\n",
      "Epoch 21/69\n",
      "402788/402788 [==============================] - 748s 2ms/step - loss: 0.2676 - mae: 0.3978 - val_loss: 0.2796 - val_mae: 0.4041\n",
      "Epoch 22/69\n",
      "402788/402788 [==============================] - 749s 2ms/step - loss: 0.2676 - mae: 0.3978 - val_loss: 0.2712 - val_mae: 0.3943\n",
      "Epoch 23/69\n",
      "402788/402788 [==============================] - 753s 2ms/step - loss: 0.2676 - mae: 0.3977 - val_loss: 0.2748 - val_mae: 0.3994\n",
      "Epoch 24/69\n",
      "402788/402788 [==============================] - 752s 2ms/step - loss: 0.2677 - mae: 0.3978 - val_loss: 0.2781 - val_mae: 0.3845\n",
      "Epoch 25/69\n",
      "402788/402788 [==============================] - 755s 2ms/step - loss: 0.2676 - mae: 0.3978 - val_loss: 0.2802 - val_mae: 0.4035\n",
      "Epoch 26/69\n",
      "402788/402788 [==============================] - 754s 2ms/step - loss: 0.2676 - mae: 0.3978 - val_loss: 0.2662 - val_mae: 0.3833\n",
      "Epoch 27/69\n",
      "402788/402788 [==============================] - 783s 2ms/step - loss: 0.2675 - mae: 0.3977 - val_loss: 0.2813 - val_mae: 0.4015\n",
      "Epoch 28/69\n",
      "402788/402788 [==============================] - 759s 2ms/step - loss: 0.2675 - mae: 0.3977 - val_loss: 0.2779 - val_mae: 0.4039\n",
      "Epoch 29/69\n",
      "402788/402788 [==============================] - 765s 2ms/step - loss: 0.2676 - mae: 0.3978 - val_loss: 0.2818 - val_mae: 0.4042\n",
      "Epoch 30/69\n",
      "402788/402788 [==============================] - 765s 2ms/step - loss: 0.2677 - mae: 0.3978 - val_loss: 0.2720 - val_mae: 0.3957\n",
      "Epoch 31/69\n",
      "402788/402788 [==============================] - 772s 2ms/step - loss: 0.2677 - mae: 0.3978 - val_loss: 0.2667 - val_mae: 0.3826\n",
      "Epoch 32/69\n",
      "402788/402788 [==============================] - 763s 2ms/step - loss: 0.2675 - mae: 0.3977 - val_loss: 0.2688 - val_mae: 0.3886\n",
      "Epoch 33/69\n",
      "402788/402788 [==============================] - 758s 2ms/step - loss: 0.2676 - mae: 0.3978 - val_loss: 0.2692 - val_mae: 0.3911\n",
      "Epoch 34/69\n",
      "402788/402788 [==============================] - 776s 2ms/step - loss: 0.2675 - mae: 0.3977 - val_loss: 0.3015 - val_mae: 0.3956\n",
      "Epoch 35/69\n",
      "402788/402788 [==============================] - 765s 2ms/step - loss: 0.2675 - mae: 0.3977 - val_loss: 0.2739 - val_mae: 0.3841\n",
      "Epoch 36/69\n",
      "402788/402788 [==============================] - 751s 2ms/step - loss: 0.2676 - mae: 0.3977 - val_loss: 0.2991 - val_mae: 0.3971\n",
      "Epoch 37/69\n",
      "402788/402788 [==============================] - 740s 2ms/step - loss: 0.2676 - mae: 0.3977 - val_loss: 0.2684 - val_mae: 0.3828\n",
      "Epoch 38/69\n",
      "402788/402788 [==============================] - 785s 2ms/step - loss: 0.2676 - mae: 0.3977 - val_loss: 0.2774 - val_mae: 0.3897\n",
      "Epoch 39/69\n",
      "402788/402788 [==============================] - 766s 2ms/step - loss: 0.2675 - mae: 0.3977 - val_loss: 0.2700 - val_mae: 0.3835\n",
      "Epoch 40/69\n",
      "402788/402788 [==============================] - 754s 2ms/step - loss: 0.2676 - mae: 0.3977 - val_loss: 0.2702 - val_mae: 0.3848\n",
      "Epoch 41/69\n",
      "402788/402788 [==============================] - 751s 2ms/step - loss: 0.2677 - mae: 0.3979 - val_loss: 0.2759 - val_mae: 0.3846\n",
      "Epoch 42/69\n",
      "402788/402788 [==============================] - 766s 2ms/step - loss: 0.2675 - mae: 0.3977 - val_loss: 0.2693 - val_mae: 0.3854\n",
      "Epoch 43/69\n",
      "402788/402788 [==============================] - 771s 2ms/step - loss: 0.2676 - mae: 0.3977 - val_loss: 0.2683 - val_mae: 0.3886\n",
      "Epoch 44/69\n",
      "402788/402788 [==============================] - 799s 2ms/step - loss: 0.2675 - mae: 0.3977 - val_loss: 0.2717 - val_mae: 0.3924\n",
      "Epoch 45/69\n",
      "402788/402788 [==============================] - 776s 2ms/step - loss: 0.2675 - mae: 0.3977 - val_loss: 0.3171 - val_mae: 0.4421\n",
      "Epoch 46/69\n",
      "402788/402788 [==============================] - 756s 2ms/step - loss: 0.2676 - mae: 0.3978 - val_loss: 0.2834 - val_mae: 0.3907\n",
      "Epoch 47/69\n",
      "402788/402788 [==============================] - 756s 2ms/step - loss: 0.2675 - mae: 0.3977 - val_loss: 0.2734 - val_mae: 0.3940\n",
      "Epoch 48/69\n",
      "402788/402788 [==============================] - 760s 2ms/step - loss: 0.2676 - mae: 0.3978 - val_loss: 0.2720 - val_mae: 0.3837\n",
      "Epoch 49/69\n",
      "402788/402788 [==============================] - 763s 2ms/step - loss: 0.2675 - mae: 0.3977 - val_loss: 0.2709 - val_mae: 0.3941\n",
      "Epoch 50/69\n",
      "402788/402788 [==============================] - 763s 2ms/step - loss: 0.2676 - mae: 0.3978 - val_loss: 0.2649 - val_mae: 0.3842\n",
      "Epoch 51/69\n",
      "402788/402788 [==============================] - 773s 2ms/step - loss: 0.2676 - mae: 0.3977 - val_loss: 0.2705 - val_mae: 0.3899\n",
      "Epoch 52/69\n",
      "402788/402788 [==============================] - 769s 2ms/step - loss: 0.2676 - mae: 0.3978 - val_loss: 0.3359 - val_mae: 0.4172\n",
      "Epoch 53/69\n",
      "402788/402788 [==============================] - 761s 2ms/step - loss: 0.2676 - mae: 0.3977 - val_loss: 0.2780 - val_mae: 0.3999\n",
      "Epoch 54/69\n",
      "402788/402788 [==============================] - 752s 2ms/step - loss: 0.2676 - mae: 0.3978 - val_loss: 0.2842 - val_mae: 0.4078\n",
      "Epoch 55/69\n",
      "402788/402788 [==============================] - 764s 2ms/step - loss: 0.2676 - mae: 0.3978 - val_loss: 0.2739 - val_mae: 0.3932\n",
      "Epoch 56/69\n",
      "402788/402788 [==============================] - 775s 2ms/step - loss: 0.2675 - mae: 0.3977 - val_loss: 0.2703 - val_mae: 0.3828\n",
      "Epoch 57/69\n",
      "402788/402788 [==============================] - 756s 2ms/step - loss: 0.2675 - mae: 0.3977 - val_loss: 0.2692 - val_mae: 0.3859\n",
      "Epoch 58/69\n",
      "402788/402788 [==============================] - 869s 2ms/step - loss: 0.2676 - mae: 0.3977 - val_loss: 0.2867 - val_mae: 0.3877\n",
      "Epoch 59/69\n",
      "402788/402788 [==============================] - 1351s 3ms/step - loss: 0.2675 - mae: 0.3977 - val_loss: 0.2819 - val_mae: 0.3859\n",
      "Epoch 60/69\n",
      "402788/402788 [==============================] - 3761s 9ms/step - loss: 0.2677 - mae: 0.3978 - val_loss: 0.2762 - val_mae: 0.3842\n",
      "Epoch 61/69\n",
      "402788/402788 [==============================] - 519s 1ms/step - loss: 0.2676 - mae: 0.3978 - val_loss: 0.2702 - val_mae: 0.3826\n",
      "Epoch 62/69\n",
      "402788/402788 [==============================] - 427s 1ms/step - loss: 0.2676 - mae: 0.3977 - val_loss: 0.2829 - val_mae: 0.3875\n",
      "Epoch 63/69\n",
      "402788/402788 [==============================] - 475s 1ms/step - loss: 0.2676 - mae: 0.3978 - val_loss: 0.3056 - val_mae: 0.4076\n",
      "Epoch 64/69\n",
      "402788/402788 [==============================] - 517s 1ms/step - loss: 0.2676 - mae: 0.3978 - val_loss: 0.3140 - val_mae: 0.4395\n",
      "Epoch 65/69\n",
      "402788/402788 [==============================] - 759s 2ms/step - loss: 0.2676 - mae: 0.3977 - val_loss: 0.2696 - val_mae: 0.3825\n",
      "Epoch 66/69\n",
      "402788/402788 [==============================] - 694s 2ms/step - loss: 0.2676 - mae: 0.3978 - val_loss: 0.2693 - val_mae: 0.3930\n",
      "Epoch 67/69\n",
      "402788/402788 [==============================] - 663s 2ms/step - loss: 0.2675 - mae: 0.3977 - val_loss: 0.2740 - val_mae: 0.3981\n",
      "Epoch 68/69\n",
      "402788/402788 [==============================] - 1240s 3ms/step - loss: 0.2677 - mae: 0.3978 - val_loss: 0.2676 - val_mae: 0.3900\n",
      "Epoch 69/69\n",
      "402788/402788 [==============================] - 944s 2ms/step - loss: 0.2675 - mae: 0.3977 - val_loss: 0.2821 - val_mae: 0.3859\n"
     ]
    }
   ],
   "source": [
    "single_nn = neural_net(\n",
    "    x_train,\n",
    "    y_train,\n",
    "    learning_rate = 0.05,\n",
    "    n_hidden_units = n_hidden_units,\n",
    "    epochs = epochs,\n",
    "    validation_data = (x_dev, y_dev)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "single_nn.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Deep Neural Network\n",
    "\n",
    "The full model is specified as a deep neural network using layers with ReLU activation functions with a linear activation in the output layer. The choice of number of layers was initially kept small due to computational processing constraints."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/69\n",
      "402788/402788 [==============================] - 1678s 4ms/step - loss: 0.2217 - mae: 0.3546 - val_loss: 0.1814 - val_mae: 0.3223\n",
      "Epoch 2/69\n",
      "402788/402788 [==============================] - 1375s 3ms/step - loss: 0.1995 - mae: 0.3426 - val_loss: 0.1802 - val_mae: 0.3220\n",
      "Epoch 3/69\n",
      "402788/402788 [==============================] - 1445s 4ms/step - loss: 0.1979 - mae: 0.3412 - val_loss: 0.1805 - val_mae: 0.3206\n",
      "Epoch 4/69\n",
      "402788/402788 [==============================] - 1510s 4ms/step - loss: 0.1969 - mae: 0.3403 - val_loss: 0.1804 - val_mae: 0.3209\n",
      "Epoch 5/69\n",
      "402788/402788 [==============================] - 1265s 3ms/step - loss: 0.1963 - mae: 0.3398 - val_loss: 0.1768 - val_mae: 0.3206\n",
      "Epoch 6/69\n",
      "402788/402788 [==============================] - 1418s 4ms/step - loss: 0.1959 - mae: 0.3394 - val_loss: 0.1752 - val_mae: 0.3194\n",
      "Epoch 7/69\n",
      "402788/402788 [==============================] - 2101s 5ms/step - loss: 0.1956 - mae: 0.3392 - val_loss: 0.1838 - val_mae: 0.3304\n",
      "Epoch 8/69\n",
      "402788/402788 [==============================] - 1614s 4ms/step - loss: 0.1954 - mae: 0.3390 - val_loss: 0.1820 - val_mae: 0.3271\n",
      "Epoch 9/69\n",
      "402788/402788 [==============================] - 2266s 6ms/step - loss: 0.1953 - mae: 0.3389 - val_loss: 0.1863 - val_mae: 0.3349\n",
      "Epoch 10/69\n",
      "402788/402788 [==============================] - 1588s 4ms/step - loss: 0.1951 - mae: 0.3388 - val_loss: 0.1765 - val_mae: 0.3194\n",
      "Epoch 11/69\n",
      "402788/402788 [==============================] - 1546s 4ms/step - loss: 0.1950 - mae: 0.3387 - val_loss: 0.1788 - val_mae: 0.3250\n",
      "Epoch 12/69\n",
      "402788/402788 [==============================] - 1563s 4ms/step - loss: 0.1949 - mae: 0.3385 - val_loss: 0.1807 - val_mae: 0.3202\n",
      "Epoch 13/69\n",
      "402788/402788 [==============================] - 1627s 4ms/step - loss: 0.1948 - mae: 0.3384 - val_loss: 0.1801 - val_mae: 0.3301\n",
      "Epoch 14/69\n",
      "402788/402788 [==============================] - 1285s 3ms/step - loss: 0.1947 - mae: 0.3383 - val_loss: 0.1811 - val_mae: 0.3254\n",
      "Epoch 15/69\n",
      "402788/402788 [==============================] - 1445s 4ms/step - loss: 0.1944 - mae: 0.3381 - val_loss: 0.1762 - val_mae: 0.3255\n",
      "Epoch 16/69\n",
      "402788/402788 [==============================] - 1486s 4ms/step - loss: 0.1943 - mae: 0.3380 - val_loss: 0.1800 - val_mae: 0.3251\n",
      "Epoch 17/69\n",
      "402788/402788 [==============================] - 1412s 4ms/step - loss: 0.1942 - mae: 0.3379 - val_loss: 0.1770 - val_mae: 0.3194\n",
      "Epoch 18/69\n",
      "402788/402788 [==============================] - 1925s 5ms/step - loss: 0.1941 - mae: 0.3378 - val_loss: 0.1750 - val_mae: 0.3183\n",
      "Epoch 19/69\n",
      "402788/402788 [==============================] - 2267s 6ms/step - loss: 0.1940 - mae: 0.3378 - val_loss: 0.1827 - val_mae: 0.3257\n",
      "Epoch 20/69\n",
      "402788/402788 [==============================] - 2036s 5ms/step - loss: 0.1939 - mae: 0.3377 - val_loss: 0.1781 - val_mae: 0.3193\n",
      "Epoch 21/69\n",
      "402788/402788 [==============================] - 2215s 5ms/step - loss: 0.1939 - mae: 0.3377 - val_loss: 0.1830 - val_mae: 0.3356\n",
      "Epoch 22/69\n",
      "402788/402788 [==============================] - 2148s 5ms/step - loss: 0.1938 - mae: 0.3376 - val_loss: 0.1818 - val_mae: 0.3207\n",
      "Epoch 23/69\n",
      "402788/402788 [==============================] - 2155s 5ms/step - loss: 0.1938 - mae: 0.3376 - val_loss: 0.1799 - val_mae: 0.3289\n",
      "Epoch 24/69\n",
      "402788/402788 [==============================] - 2176s 5ms/step - loss: 0.1938 - mae: 0.3375 - val_loss: 0.1780 - val_mae: 0.3262\n",
      "Epoch 25/69\n",
      "402788/402788 [==============================] - 2175s 5ms/step - loss: 0.1937 - mae: 0.3375 - val_loss: 0.1759 - val_mae: 0.3246\n",
      "Epoch 26/69\n",
      "402788/402788 [==============================] - 2188s 5ms/step - loss: 0.1936 - mae: 0.3374 - val_loss: 0.1743 - val_mae: 0.3179\n",
      "Epoch 27/69\n",
      "402788/402788 [==============================] - 2190s 5ms/step - loss: 0.1936 - mae: 0.3374 - val_loss: 0.1774 - val_mae: 0.3194\n",
      "Epoch 28/69\n",
      "402788/402788 [==============================] - 2168s 5ms/step - loss: 0.1936 - mae: 0.3373 - val_loss: 0.1784 - val_mae: 0.3204\n",
      "Epoch 29/69\n",
      "402788/402788 [==============================] - 2131s 5ms/step - loss: 0.1934 - mae: 0.3373 - val_loss: 0.1778 - val_mae: 0.3236\n",
      "Epoch 30/69\n",
      "402788/402788 [==============================] - 2022s 5ms/step - loss: 0.1935 - mae: 0.3373 - val_loss: 0.1782 - val_mae: 0.3265\n",
      "Epoch 31/69\n",
      "402788/402788 [==============================] - 2066s 5ms/step - loss: 0.1935 - mae: 0.3373 - val_loss: 0.1808 - val_mae: 0.3294\n",
      "Epoch 32/69\n",
      "402788/402788 [==============================] - 2024s 5ms/step - loss: 0.1935 - mae: 0.3373 - val_loss: 0.1836 - val_mae: 0.3360\n",
      "Epoch 33/69\n",
      "402788/402788 [==============================] - 2017s 5ms/step - loss: 0.1934 - mae: 0.3372 - val_loss: 0.1759 - val_mae: 0.3184\n",
      "Epoch 34/69\n",
      "402788/402788 [==============================] - 2018s 5ms/step - loss: 0.1934 - mae: 0.3372 - val_loss: 0.1762 - val_mae: 0.3215\n",
      "Epoch 35/69\n",
      "402788/402788 [==============================] - 1970s 5ms/step - loss: 0.1934 - mae: 0.3371 - val_loss: 0.1784 - val_mae: 0.3289\n",
      "Epoch 36/69\n",
      "402788/402788 [==============================] - 1984s 5ms/step - loss: 0.1933 - mae: 0.3371 - val_loss: 0.1739 - val_mae: 0.3197\n",
      "Epoch 37/69\n",
      "402788/402788 [==============================] - 2028s 5ms/step - loss: 0.1934 - mae: 0.3372 - val_loss: 0.1783 - val_mae: 0.3249\n",
      "Epoch 38/69\n",
      "402788/402788 [==============================] - 2020s 5ms/step - loss: 0.1933 - mae: 0.3371 - val_loss: 0.1833 - val_mae: 0.3295\n",
      "Epoch 39/69\n",
      "402788/402788 [==============================] - 1919s 5ms/step - loss: 0.1934 - mae: 0.3372 - val_loss: 0.1760 - val_mae: 0.3238\n",
      "Epoch 40/69\n",
      "402788/402788 [==============================] - 1750s 4ms/step - loss: 0.1933 - mae: 0.3371 - val_loss: 0.1768 - val_mae: 0.3238\n",
      "Epoch 41/69\n",
      "402788/402788 [==============================] - 1762s 4ms/step - loss: 0.1933 - mae: 0.3371 - val_loss: 0.1759 - val_mae: 0.3191\n",
      "Epoch 42/69\n",
      "402788/402788 [==============================] - 1814s 5ms/step - loss: 0.1933 - mae: 0.3371 - val_loss: 0.1792 - val_mae: 0.3295\n",
      "Epoch 43/69\n",
      "402788/402788 [==============================] - 9449s 23ms/step - loss: 0.1933 - mae: 0.3371 - val_loss: 0.1773 - val_mae: 0.3209\n",
      "Epoch 44/69\n",
      "402788/402788 [==============================] - 1810s 4ms/step - loss: 0.1932 - mae: 0.3370 - val_loss: 0.1771 - val_mae: 0.3209\n",
      "Epoch 45/69\n",
      "402788/402788 [==============================] - 1722s 4ms/step - loss: 0.1931 - mae: 0.3370 - val_loss: 0.1886 - val_mae: 0.3383\n",
      "Epoch 46/69\n",
      "402788/402788 [==============================] - 1620s 4ms/step - loss: 0.1932 - mae: 0.3371 - val_loss: 0.1799 - val_mae: 0.3194\n",
      "Epoch 47/69\n",
      "402788/402788 [==============================] - 1731s 4ms/step - loss: 0.1932 - mae: 0.3370 - val_loss: 0.1832 - val_mae: 0.3326\n",
      "Epoch 48/69\n",
      "402788/402788 [==============================] - 1741s 4ms/step - loss: 0.1932 - mae: 0.3370 - val_loss: 0.1757 - val_mae: 0.3191\n",
      "Epoch 49/69\n",
      "402788/402788 [==============================] - 1691s 4ms/step - loss: 0.1932 - mae: 0.3370 - val_loss: 0.1777 - val_mae: 0.3237\n",
      "Epoch 50/69\n",
      "402788/402788 [==============================] - 1677s 4ms/step - loss: 0.1931 - mae: 0.3370 - val_loss: 0.1765 - val_mae: 0.3209\n",
      "Epoch 51/69\n",
      "402788/402788 [==============================] - 1749s 4ms/step - loss: 0.1931 - mae: 0.3369 - val_loss: 0.2923 - val_mae: 0.4180\n",
      "Epoch 52/69\n",
      " 60739/402788 [===>..........................] - ETA: 31:07 - loss: 0.1934 - mae: 0.3370"
     ]
    }
   ],
   "source": [
    "deep_nn = neural_net(\n",
    "    x_train,\n",
    "    y_train,\n",
    "    n_layers = 10,\n",
    "    n_hidden_units = n_hidden_units,\n",
    "    epochs = epochs,\n",
    "    validation_data = (x_dev, y_dev)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "deep_nn.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scikeras.wrappers import KerasRegressor\n",
    "from sklearn.model_selection import RandomizedSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def neural_net_model(\n",
    "        x_train: Union[tf.Tensor, np.ndarray],\n",
    "        n_hidden_units: int,\n",
    "        n_layers: int = 1,\n",
    "        learning_rate: float = 0.01,\n",
    "        hidden_activation: str = 'relu',\n",
    "        output_activation: str = 'linear',\n",
    "        loss: str = 'mean_squared_error',\n",
    "    ) -> tf.keras.Model:\n",
    "    \"\"\"\n",
    "    This function creates a neural network model with n_layers hidden layers\n",
    "    and an output layer using the activations specified.\n",
    "    \"\"\"\n",
    "\n",
    "    model = models.Sequential()\n",
    "    model.add(layers.InputLayer(input_shape=x_train.shape[1:]))\n",
    "    \n",
    "    # Adding hidden layers\n",
    "    for _ in range(n_layers):\n",
    "        model.add(\n",
    "            layers.Dense(\n",
    "                n_hidden_units,\n",
    "                kernel_initializer=initializers.he_normal(),\n",
    "                trainable=True)\n",
    "            )\n",
    "        model.add(layers.BatchNormalization())\n",
    "        model.add(layers.Activation(hidden_activation))\n",
    "    \n",
    "    # Adding output layer\n",
    "    model.add(layers.Dense(\n",
    "        units=1,\n",
    "        activation=output_activation,\n",
    "        kernel_initializer=initializers.glorot_normal(),\n",
    "        trainable=True))\n",
    "    \n",
    "    model.compile(\n",
    "        optimizer=tf._optimizers.Adam(learning_rate=learning_rate),\n",
    "        loss=loss\n",
    "    )\n",
    "    \n",
    "    model.fit(x_train, y_train, epochs=10, batch_size=32, verbose=0)\n",
    "\n",
    "    return model\n",
    "\n",
    "# Create a wrapper function for the model\n",
    "model = KerasRegressor(\n",
    "    build_fn=neural_net_model,\n",
    "    x_train=x_train,\n",
    "    y_train = y_train,\n",
    "    optimizer='Adam',\n",
    "    loss = 'mean_squared_error',\n",
    "    epochs=10,\n",
    "    batch_size=32,\n",
    "    verbose=0\n",
    ")\n",
    "\n",
    "# Define hyperparameter search space\n",
    "param_dist = {\n",
    "    'n_hidden_units': [12, 18, 24],\n",
    "    'n_layers': [5,8,10],\n",
    "    'learning_rate': [0.001, 0.01, 0.1]\n",
    "}\n",
    "\n",
    "random_search = RandomizedSearchCV(\n",
    "    model,\n",
    "    param_distributions=param_dist,\n",
    "    n_iter=10,\n",
    "    scoring='neg_mean_squared_error',\n",
    ")\n",
    "random_search_result = random_search.fit(x_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_search_result = random_search.fit(x_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nn_dict = {\n",
    "    'NN - 1 layer': single_nn.history.history['loss'],\n",
    "    'NN - 5 layers': deep_nn.history.history['loss']\n",
    "}\n",
    "baseline_dict = {\n",
    "    'Random Forest': rfr_mse,\n",
    "    # 'Random Forest (tuned)': rfr_t_mse,\n",
    "    'XGBoost': xgb_mse\n",
    "}\n",
    "\n",
    "generate_plot(nn_dict, baseline_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculating marginal valuation of environmental attributes\n",
    "\n",
    "Once we have a model with environmental attributes as features and log of house price as the target variable, we can begin to construct the marginal willingness to pay for environmental attributes, and therefore get a proxy for their value.\n",
    "\n",
    "The partial derivative of `ln_price` w.r.t an input variable can be evaluated during the backwards propagation stage of the model fitting. By evaluating this derivative at a variety of samples, we can plot a valulation curve for % change in house price resulting from a percentile change in the rank of a house for a given environmental attribute."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract the gradients from the model\n",
    "points = np.arange(0, 1.05, 0.05)\n",
    "gradients = calc_partial_grad(\n",
    "    deep_nn,\n",
    "    x_train,\n",
    "    derivative_index,\n",
    "    points)\n",
    "\n",
    "# calculate gradients across 20 different points (per attribute)\n",
    "\n",
    "# plot the derived valuation curves"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Saving models\n",
    "\n",
    "Here we save the pre-trained models so they can be loaded and used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_dir = cwd / \"outputs\" / \"models\"\n",
    "\n",
    "# save models as tensor objects\n",
    "deep_nn.save(model_dir / \"nn_layers_5\")\n",
    "single_nn.save(model_dir / \"nn_layers_1\")\n",
    "\n",
    "# save models as joblib files\n",
    "joblib.dump(rfr_model, model_dir / \"random_forest.joblib\")\n",
    "# joblib.dump(rfr_t_model, model_dir / \"random_forest_tuned.joblib\")\n",
    "joblib.dump(xgb_model, model_dir / \"xgboost.joblib\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.1"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
