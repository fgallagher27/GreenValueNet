{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GreenValueNet\n",
    "\n",
    "This notebook contains the code needed to execute the GreenValueNet hedonic pricing neural network. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set up and data loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Python310\\lib\\site-packages\\scipy\\__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.26.2\n",
      "  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\Python310\\lib\\site-packages\\keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from data_load_funcs import get_params, load_data_catalogue\n",
    "from processing_funcs import process_data, normalise_values\n",
    "from model_funcs import create_x_y_arr, split_to_test_dev_train, random_forest_reg, baseline_nn\n",
    "\n",
    "params = get_params()\n",
    "data_catalogue = load_data_catalogue()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you do not have a file called `dataset.csv` in the `data/interim_files` folder the following cell will generate this folder and generate summary statistics. The data processing happens locally and invovles large datasets with spatial components so can take quite several hours - please be pateint! If you already have the file, it will be read in and summary statistics are generated. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed dataset already exists.\n",
      "Loading exisitng dataset...\n",
      "                                  Mean     Std Dev        Maximum   Minimum\n",
      "ln_price                     12.315576    0.671025      18.134158  0.116855\n",
      "propertytype                  1.330222    1.086272       3.000000  0.000000\n",
      "oldnew                        0.180940    0.384968       1.000000  0.000000\n",
      "duration                      0.344295    0.475170       2.000000  0.000000\n",
      "current_energy_efficiency    62.324093   13.226019     347.000000  0.000000\n",
      "potential_energy_efficiency   0.811409    0.144810      70.000000  0.000000\n",
      "total_floor_area             91.697405  139.792843  530331.552000  0.000000\n",
      "extension_count               0.488791    0.732426       4.000000  0.000000\n",
      "number_habitable_rooms        4.545110    1.703158     137.000000  0.000000\n",
      "number_heated_rooms           4.456910    1.712974     100.000000  0.000000\n",
      "construction_age_band         4.998326    3.503951      12.000000 -1.000000\n",
      "coastline_dist               60.271371   36.139792     135.732016  0.004453\n",
      "prim_school_dist              0.770413    0.612237      19.868981  0.000000\n",
      "sec_school_dist               2.115787    2.063565      38.699688  0.000000\n",
      "roads_dist                   17.065544   25.703697     205.721758  0.031297\n",
      "nat_park_dist                77.159722   44.709009     192.706041  0.000000\n",
      "nat_trust_dist               23.051302   18.923822     142.965575  0.000000\n",
      "ttwa_dist                     0.000111    0.004716       0.567089  0.000000\n",
      "dom_builds_share              0.074511    0.052999       0.311189  0.000168\n",
      "garden_share                  0.209138    0.133022       0.628869  0.000482\n",
      "non_dom_builds_share          0.038293    0.042055       0.421751  0.000082\n",
      "path_share                    0.007069    0.006582       0.070791  0.000003\n",
      "greenspace_share              0.471083    0.270221       0.987128  0.012036\n",
      "water_share                   0.028075    0.081748       0.871968  0.000000\n"
     ]
    }
   ],
   "source": [
    "dataset = process_data(data_catalogue, params)\n",
    "\n",
    "# show summary stats\n",
    "summary_stats = dataset.describe().transpose()[['mean', 'std', 'max', 'min']]\n",
    "summary_stats.columns = ['Mean', 'Std Dev', 'Maximum', 'Minimum']\n",
    "print(summary_stats)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we normalise any non-encoded variables to increase speed of learning of algorithm and convert the dataset to an array of inputs, and an associated output array."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "norm_cols = [col for col in dataset.columns if col not in params['non_norm_cols']]\n",
    "for col in norm_cols:\n",
    "    dataset[col] = normalise_values(dataset[col])\n",
    "\n",
    "x, y = create_x_y_arr(dataset, params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['transactionid', 'ln_price', 'postcode', 'propertytype', 'oldnew',\n",
       "       'duration', 'current_energy_efficiency', 'potential_energy_efficiency',\n",
       "       'total_floor_area', 'extension_count', 'number_habitable_rooms',\n",
       "       'number_heated_rooms', 'construction_age_band', 'coastline_dist',\n",
       "       'prim_school_dist', 'sec_school_dist', 'roads_dist', 'nat_park_dist',\n",
       "       'nat_trust_dist', 'ttwa_dist', 'dom_builds_share', 'garden_share',\n",
       "       'non_dom_builds_share', 'path_share', 'greenspace_share',\n",
       "       'water_share'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# things to check:\n",
    "# max house price, floor area, current energy efficinecy, no. rooms, road dist\n",
    "# min floor area, habitable rooms, heated rooms, construction age band\n",
    "\n",
    "# do we want to remove any outliers? what should we be doing with blank data? is it treated as 0?\n",
    "dataset.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dataset is then split into train, dev and test sets using sci-kit learn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, x_dev, x_test, y_train, y_dev, y_test = split_to_test_dev_train(\n",
    "    x,\n",
    "    y,\n",
    "    params['dev_size'],\n",
    "    params['test_size'],\n",
    "    prop=False\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Benchmarking\n",
    "\n",
    "To evaluate the performance of my neural network I will run random forest and XGBoost regressions as baseline models. I will then build 2 alternative models: a deep neural network and a bayesian model. We optimise based on the mean squared error (MSE) but and report both mean squared and root mean squared errors (RMSE). By minimising the MSE, we have necessarily minimise the RMSE but with less computation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Forest\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run baseline random forest regression using scikit-learn\n",
    "rfr_model = random_forest_reg(\n",
    "    x_train,\n",
    "    x_dev,\n",
    "    y_train,\n",
    "    y_dev,\n",
    "    tuning=False\n",
    ")\n",
    "\n",
    "# now run with grid search to tune parameters\n",
    "rfr_tuned  =  random_forest_reg(\n",
    "    x_train,\n",
    "    x_dev,\n",
    "    y_train,\n",
    "    y_dev,\n",
    "    tuning=True,\n",
    "    tuning_params = params['tuning_dict']['grid']\n",
    ")\n",
    "\n",
    "# generate predictions and measure according to mean squared error\n",
    "rfr_pred = rfr_model.predict(x_dev)\n",
    "mse_rfr = mean_squared_error(y_dev, rfr_pred)\n",
    "rmse_rfr = mse_rfr ** 0.5\n",
    "\n",
    "rfr_t_pred = rfr_tuned.predict(x_dev)\n",
    "mse_t_rfr = mean_squared_error(y_dev, rfr_t_pred)\n",
    "rmse_t_rfr = mse_t_rfr ** 0.5\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neural networks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Single Layer Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fitted_single_nn = baselin_nn()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Deep Neural Network\n",
    "\n",
    "The full model is specified as a deep neural network using layers with ReLU activation functions with a linear activation in the output layer. The choice of number of layers was initially kept small due to computational processing constraints."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "deep_nn = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save model as tensor object and stick in a folder called outputs"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.1"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
