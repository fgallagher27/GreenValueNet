{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GreenValueNet\n",
    "\n",
    "This notebook contains the code needed to execute the GreenValueNet hedonic pricing neural network. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set up and data loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from pathlib import Path\n",
    "from data_load_funcs import get_params, load_data_catalogue\n",
    "from processing_funcs import process_data, normalise_values\n",
    "from model_funcs import split_to_test_dev_train, extract_target_var\n",
    "\n",
    "params = get_params()\n",
    "data_catalogue = load_data_catalogue()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you do not have a file called `dataset.csv` in the `data/interim_files` folder the following cell will generate this folder and generate summary statistics. The data processing happens locally and invovles large datasets with spatial components so can take quite several hours - please be pateint! If you already have the file, it will be read in and summary statistics are generated. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = process_data(data_catalogue, params)\n",
    "\n",
    "# show summary stats\n",
    "summary_stats = dataset.describe().transpose()[['mean', 'std', 'max', 'min']]\n",
    "summary_stats.columns = ['Mean', 'Std Dev', 'Maximum', 'Minimum']\n",
    "print(summary_stats)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we normalise any non-encoded variables to increase speed of learning of algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "norm_cols = [col for col in dataset.columns if col not in params['non_norm_cols']]\n",
    "for col in norm_cols:\n",
    "    dataset[col] = normalise_values(dataset[col])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# things to check:\n",
    "# max house price, floor area, current energy efficinecy, no. rooms, road dist\n",
    "# min floor area, habitable rooms, heated rooms, construction age band\n",
    "\n",
    "# do we want to remove any outliers? what should we be doing with blank data? is it treated as 0?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['transactionid', 'ln_price', 'postcode', 'propertytype', 'oldnew',\n",
       "       'duration', 'current_energy_efficiency', 'potential_energy_efficiency',\n",
       "       'total_floor_area', 'extension_count', 'number_habitable_rooms',\n",
       "       'number_heated_rooms', 'construction_age_band', 'coastline_dist',\n",
       "       'prim_school_dist', 'sec_school_dist', 'roads_dist', 'nat_park_dist',\n",
       "       'nat_trust_dist', 'ttwa_dist', 'dom_builds_share', 'garden_share',\n",
       "       'non_dom_builds_share', 'path_share', 'greenspace_share',\n",
       "       'water_share'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dataset is then split into train, dev and test sets, and identifying columns are dropped before being used in the models. We then isolate the target variable `ln_price` from the input variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train, dev, test = split_to_test_dev_train(\n",
    "    dataset,\n",
    "    params['dev_size'],\n",
    "    params['test_size'],\n",
    "    prop=False\n",
    ")\n",
    "\n",
    "x_train, y_train = extract_target_var(train, params['target_var'], params['cols_out'])\n",
    "x_dev, y_dev = extract_target_var(dev, params['target_var'], params['cols_out'])\n",
    "x_test, y_test = extract_target_var(test, params['target_var'], params['cols_out'])\n",
    "\n",
    "# drop any non numeric cols here\n",
    "dataset_tf = tf.convert_to_tensor(dataset)\n",
    "\n",
    "\n",
    "# maybe convert into a matrix with clear labelling for computational processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Benchmarking\n",
    "\n",
    "To evaluate the performance of my neural network I will run a single layer NN and a random forest as baseline models. I will then build 2 alternative models: a deep neural network and a bayesian model. We optimise based on the mean squared error (MSE) but and report both mean squared and root mean squared errors (RMSE). By minimising the MSE, we have necessarily minimise the RMSE but with less computation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Forest\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "\n",
    "rfr = RandomForestRegressor().fit(x_train, y_train)\n",
    "rfr_pred = rfr.predict(x_dev)\n",
    "mse = mean_squared_error(y_dev, rfr_pred)\n",
    "rmse = mse**.5\n",
    "\n",
    "# Random forest regressor parameter tuning\n",
    "# TODO move to parameters\n",
    "grid = {\n",
    "    'n_estimators': [200, 300, 400, 500],\n",
    "    'max_features': ['sqrt', 'log2'],\n",
    "    'max_depth': [3,4,5,6,7],\n",
    "    'random_state': [18]\n",
    "\n",
    "}\n",
    "CV_rfr = GridSearchCV(\n",
    "    estimator=RandomForestRegressor(),\n",
    "    param_frid=grid, cv=5\n",
    ")\n",
    "CV_rfr.fit(x_train, y_train)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Single Layer Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set up single layer NN\n",
    "from tensorflow.keras.metrics import mean_squared_error\n",
    "\n",
    "normalizer = tf.keras.layers.Normalization(axis=-1)\n",
    "normalizer.adapt(dataset_tf)\n",
    "\n",
    "def get_basic_model():\n",
    "  model = tf.keras.Sequential([\n",
    "    normalizer,\n",
    "    tf.keras.layers.Dense(1, activation='linear')\n",
    "  ])\n",
    "\n",
    "  model.compile(optimizer='adam',\n",
    "                loss=tf.keras.losses.MeanSquaredError(from_logits=True),\n",
    "                metrics=['mean_squared_error'])\n",
    "  return model\n",
    "\n",
    "single_nn = get_basic_model()\n",
    "single_nn.fit(numeric_features, target, epochs=15, batch_size = BATCH_SIZE)\n",
    "\n",
    "# normally use a linear activation function (i.e. here a linear regression)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Full model\n",
    "\n",
    "### Deep Neural Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bayesian model\n",
    "\n",
    "Here I use the pymc library to build a bayesian model."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.1"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
