{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GreenValueNet\n",
    "\n",
    "This notebook contains the code needed to execute the GreenValueNet hedonic pricing neural network. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set up and data loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "from data_load_funcs import get_params, load_data_catalogue\n",
    "from processing_funcs import process_data, normalise_values\n",
    "\n",
    "cwd = Path.cwd()\n",
    "params = get_params()\n",
    "data_catalogue = load_data_catalogue()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you do not have a file called `dataset.csv` in the `data/interim_files` folder the following cell will generate this folder and generate summary statistics. The data processing happens locally and invovles large datasets with spatial components so can take quite several hours - please be pateint! If you already have the file, it will be read in and summary statistics are generated. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = process_data(data_catalogue, params)\n",
    "\n",
    "# show summary stats\n",
    "summary_stats = dataset.describe().transpose()[['mean', 'std', 'max', 'min']]\n",
    "summary_stats.columns = ['Mean', 'Std Dev', 'Maximum', 'Minimum']\n",
    "print(summary_stats)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we normalise any non-encoded variables to increase speed of learning of algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "norm_cols = [col for col in dataset.columns if col not in params['non_norm_cols']]\n",
    "for col in norm_cols:\n",
    "    dataset[col] = normalise_values(dataset[col])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# things to check:\n",
    "# max house price, floor area, current energy efficinecy, no. rooms, road dist\n",
    "# min floor area, habitable rooms, heated rooms, construction age band\n",
    "\n",
    "# do we want to remove any outliers? what should we be doing with blank data? is it treated as 0?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split into train, dev and test datasets\n",
    "import tensorflow as tf\n",
    "observations = len(dataset)\n",
    "\n",
    "# drop any non numeric cols here\n",
    "dataset_tf = tf.convert_to_tensor(dataset)\n",
    "\n",
    "# put train dev test split into params\n",
    "\n",
    "# maybe convert into a matrix with clear labelling for computational processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Benchmarking\n",
    "\n",
    "To evaluate the performance of my neural network I will run a single layer NN and a random forest as baseline line models. I will then build 2 alternative models: a deep neural network and a bayesian model. We optimise based on the mean squared error (MSE) but and report both mean squared and root mean squared errors (RMSE). By minimising the MSE, we have necessarily minimsied the RMSE but with less computation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Forest\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "\n",
    "rfr = RandomForestRegressor().fit(x_train, y_train)\n",
    "rfr_pred = rfr.predict(x_dev)\n",
    "mse = mean_squared_error(y_dev, rfr_pred)\n",
    "rmse = mse**.5\n",
    "\n",
    "# Random forest regressor parameter tuning\n",
    "grid = {\n",
    "    'n_estimators': [200, 300, 400, 500],\n",
    "    'max_features': ['sqrt', 'log2'],\n",
    "    'max_depth': [3,4,5,6,7],\n",
    "    'random_state': [18]\n",
    "\n",
    "}\n",
    "CV_rfr = GridSearchCV(\n",
    "    estimator=RandomForestRegressor(),\n",
    "    param_frid=grid, cv=5\n",
    ")\n",
    "CV_rfr.fit(x_train, y_train)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Single Layer Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set up single layer NN\n",
    "from tensorflow.keras.metrics import mean_squared_error\n",
    "\n",
    "normalizer = tf.keras.layers.Normalization(axis=-1)\n",
    "normalizer.adapt(dataset_tf)\n",
    "\n",
    "def get_basic_model():\n",
    "  model = tf.keras.Sequential([\n",
    "    normalizer,\n",
    "    tf.keras.layers.Dense(10, activation='relu'),\n",
    "    tf.keras.layers.Dense(10, activation='relu'),\n",
    "    tf.keras.layers.Dense(1)\n",
    "  ])\n",
    "\n",
    "  model.compile(optimizer='adam',\n",
    "                loss=tf.keras.losses.MeanSquaredError(from_logits=True),\n",
    "                metrics=['mean_squared_error'])\n",
    "  return model\n",
    "\n",
    "single_nn = get_basic_model()\n",
    "single_nn.fit(numeric_features, target, epochs=15, batch_size = BATCH_SIZE)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.1"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
